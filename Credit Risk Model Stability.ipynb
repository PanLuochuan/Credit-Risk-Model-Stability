{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "584dc88f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T15:48:44.252943Z",
     "iopub.status.busy": "2024-05-23T15:48:44.252577Z",
     "iopub.status.idle": "2024-05-23T15:48:50.152535Z",
     "shell.execute_reply": "2024-05-23T15:48:50.151585Z"
    },
    "papermill": {
     "duration": 5.911647,
     "end_time": "2024-05-23T15:48:50.155280",
     "exception": false,
     "start_time": "2024-05-23T15:48:44.243633",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import lightgbm as lgb  # type: ignore\n",
    "import numpy as np  # type: ignore\n",
    "import pandas as pd  # type: ignore\n",
    "import polars as pl  # type: ignore\n",
    "import warnings\n",
    "\n",
    "from catboost import CatBoostClassifier, Pool  # type: ignore\n",
    "from glob import glob\n",
    "from IPython.display import display  # type: ignore\n",
    "from pathlib import Path\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin  # type: ignore\n",
    "from sklearn.metrics import roc_auc_score  # type: ignore\n",
    "from sklearn.model_selection import StratifiedGroupKFold  # type: ignore\n",
    "from typing import Any\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "ROOT = Path(\"/kaggle/input/home-credit-credit-risk-model-stability\")\n",
    "TRAIN_DIR = ROOT / \"parquet_files\" / \"train\"\n",
    "TEST_DIR = ROOT / \"parquet_files\" / \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "277124d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T15:48:50.171495Z",
     "iopub.status.busy": "2024-05-23T15:48:50.171179Z",
     "iopub.status.idle": "2024-05-23T15:48:50.208431Z",
     "shell.execute_reply": "2024-05-23T15:48:50.207528Z"
    },
    "papermill": {
     "duration": 0.047742,
     "end_time": "2024-05-23T15:48:50.210399",
     "exception": false,
     "start_time": "2024-05-23T15:48:50.162657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Utility:\n",
    "    @staticmethod\n",
    "    def get_feat_defs(ending_with: str) -> None:\n",
    "        \"\"\"\n",
    "        Retrieves feature definitions from a CSV file based on the specified ending.\n",
    "\n",
    "        Args:\n",
    "        - ending_with (str): Ending to filter feature definitions.\n",
    "\n",
    "        Returns:\n",
    "        - pl.DataFrame: Filtered feature definitions.\n",
    "        \"\"\"\n",
    "        feat_defs: pl.DataFrame = pl.read_csv(ROOT / \"feature_definitions.csv\")\n",
    "\n",
    "        filtered_feats: pl.DataFrame = feat_defs.filter(\n",
    "            pl.col(\"Variable\").apply(lambda var: var.endswith(ending_with))\n",
    "        )\n",
    "\n",
    "        with pl.Config(fmt_str_lengths=200, tbl_rows=-1):\n",
    "            print(filtered_feats)\n",
    "\n",
    "        filtered_feats = None\n",
    "        feat_defs = None\n",
    "\n",
    "    @staticmethod\n",
    "    def find_index(lst: list[Any], item: Any) -> int | None:\n",
    "        \"\"\"\n",
    "        Finds the index of an item in a list.\n",
    "\n",
    "        Args:\n",
    "        - lst (list): List to search.\n",
    "        - item (Any): Item to find in the list.\n",
    "\n",
    "        Returns:\n",
    "        - int | None: Index of the item if found, otherwise None.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return lst.index(item)\n",
    "        except ValueError:\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def dtype_to_str(dtype: pl.DataType) -> str:\n",
    "        \"\"\"\n",
    "        Converts Polars data type to string representation.\n",
    "\n",
    "        Args:\n",
    "        - dtype (pl.DataType): Polars data type.\n",
    "\n",
    "        Returns:\n",
    "        - str: String representation of the data type.\n",
    "        \"\"\"\n",
    "        dtype_map = {\n",
    "            pl.Decimal: \"Decimal\",\n",
    "            pl.Float32: \"Float32\",\n",
    "            pl.Float64: \"Float64\",\n",
    "            pl.UInt8: \"UInt8\",\n",
    "            pl.UInt16: \"UInt16\",\n",
    "            pl.UInt32: \"UInt32\",\n",
    "            pl.UInt64: \"UInt64\",\n",
    "            pl.Int8: \"Int8\",\n",
    "            pl.Int16: \"Int16\",\n",
    "            pl.Int32: \"Int32\",\n",
    "            pl.Int64: \"Int64\",\n",
    "            pl.Date: \"Date\",\n",
    "            pl.Datetime: \"Datetime\",\n",
    "            pl.Duration: \"Duration\",\n",
    "            pl.Time: \"Time\",\n",
    "            pl.Array: \"Array\",\n",
    "            pl.List: \"List\",\n",
    "            pl.Struct: \"Struct\",\n",
    "            pl.String: \"String\",\n",
    "            pl.Categorical: \"Categorical\",\n",
    "            pl.Enum: \"Enum\",\n",
    "            pl.Utf8: \"Utf8\",\n",
    "            pl.Binary: \"Binary\",\n",
    "            pl.Boolean: \"Boolean\",\n",
    "            pl.Null: \"Null\",\n",
    "            pl.Object: \"Object\",\n",
    "            pl.Unknown: \"Unknown\",\n",
    "        }\n",
    "\n",
    "        return dtype_map.get(dtype)\n",
    "\n",
    "    @staticmethod\n",
    "    def find_feat_occur(regex_path: str, ending_with: str) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Finds occurrences of features ending with a specific string in Parquet files.\n",
    "\n",
    "        Args:\n",
    "        - regex_path (str): Regular expression to match Parquet file paths.\n",
    "        - ending_with (str): Ending to filter feature names.\n",
    "\n",
    "        Returns:\n",
    "        - pl.DataFrame: DataFrame containing feature definitions, data types, and file locations.\n",
    "        \"\"\"\n",
    "        feat_defs: pl.DataFrame = pl.read_csv(ROOT / \"feature_definitions.csv\").filter(\n",
    "            pl.col(\"Variable\").apply(lambda var: var.endswith(ending_with))\n",
    "        )\n",
    "        feat_defs.sort(by=[\"Variable\"])\n",
    "\n",
    "        feats: list[pl.String] = feat_defs[\"Variable\"].to_list()\n",
    "        feats.sort()\n",
    "\n",
    "        occurrences: list[list] = [[set(), set()] for _ in range(feat_defs.height)]\n",
    "\n",
    "        for path in glob(str(regex_path)):\n",
    "            df_schema: dict = pl.read_parquet_schema(path)\n",
    "\n",
    "            for feat, dtype in df_schema.items():\n",
    "                index: int = Utility.find_index(feats, feat)\n",
    "                if index != None:\n",
    "                    occurrences[index][0].add(Utility.dtype_to_str(dtype))\n",
    "                    occurrences[index][1].add(Path(path).stem)\n",
    "\n",
    "        data_types: list[str] = [None] * feat_defs.height\n",
    "        file_locs: list[str] = [None] * feat_defs.height\n",
    "\n",
    "        for i, feat in enumerate(feats):\n",
    "            data_types[i] = list(occurrences[i][0])\n",
    "            file_locs[i] = list(occurrences[i][1])\n",
    "\n",
    "        feat_defs = feat_defs.with_columns(pl.Series(data_types).alias(\"Data_Type(s)\"))\n",
    "        feat_defs = feat_defs.with_columns(pl.Series(file_locs).alias(\"File_Loc(s)\"))\n",
    "\n",
    "        return feat_defs\n",
    "\n",
    "    def reduce_memory_usage(df: pl.DataFrame, name) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Reduces memory usage of a DataFrame by converting column types.\n",
    "\n",
    "        Args:\n",
    "        - df (pl.DataFrame): DataFrame to optimize.\n",
    "        - name (str): Name of the DataFrame.\n",
    "\n",
    "        Returns:\n",
    "        - pl.DataFrame: Optimized DataFrame.\n",
    "        \"\"\"\n",
    "        print(\n",
    "            f\"Memory usage of dataframe \\\"{name}\\\" is {round(df.estimated_size('mb'), 4)} MB.\"\n",
    "        )\n",
    "\n",
    "        int_types = [\n",
    "            pl.Int8,\n",
    "            pl.Int16,\n",
    "            pl.Int32,\n",
    "            pl.Int64,\n",
    "            pl.UInt8,\n",
    "            pl.UInt16,\n",
    "            pl.UInt32,\n",
    "            pl.UInt64,\n",
    "        ]\n",
    "        float_types = [pl.Float32, pl.Float64]\n",
    "\n",
    "        for col in df.columns:\n",
    "            col_type = df[col].dtype\n",
    "            if col_type in int_types + float_types:\n",
    "                c_min = df[col].min()\n",
    "                c_max = df[col].max()\n",
    "\n",
    "                if c_min is not None and c_max is not None:\n",
    "                    if col_type in int_types:\n",
    "                        if c_min >= 0:\n",
    "                            if (\n",
    "                                c_min >= np.iinfo(np.uint8).min\n",
    "                                and c_max <= np.iinfo(np.uint8).max\n",
    "                            ):\n",
    "                                df = df.with_columns(df[col].cast(pl.UInt8))\n",
    "                            elif (\n",
    "                                c_min >= np.iinfo(np.uint16).min\n",
    "                                and c_max <= np.iinfo(np.uint16).max\n",
    "                            ):\n",
    "                                df = df.with_columns(df[col].cast(pl.UInt16))\n",
    "                            elif (\n",
    "                                c_min >= np.iinfo(np.uint32).min\n",
    "                                and c_max <= np.iinfo(np.uint32).max\n",
    "                            ):\n",
    "                                df = df.with_columns(df[col].cast(pl.UInt32))\n",
    "                            elif (\n",
    "                                c_min >= np.iinfo(np.uint64).min\n",
    "                                and c_max <= np.iinfo(np.uint64).max\n",
    "                            ):\n",
    "                                df = df.with_columns(df[col].cast(pl.UInt64))\n",
    "                        else:\n",
    "                            if (\n",
    "                                c_min >= np.iinfo(np.int8).min\n",
    "                                and c_max <= np.iinfo(np.int8).max\n",
    "                            ):\n",
    "                                df = df.with_columns(df[col].cast(pl.Int8))\n",
    "                            elif (\n",
    "                                c_min >= np.iinfo(np.int16).min\n",
    "                                and c_max <= np.iinfo(np.int16).max\n",
    "                            ):\n",
    "                                df = df.with_columns(df[col].cast(pl.Int16))\n",
    "                            elif (\n",
    "                                c_min >= np.iinfo(np.int32).min\n",
    "                                and c_max <= np.iinfo(np.int32).max\n",
    "                            ):\n",
    "                                df = df.with_columns(df[col].cast(pl.Int32))\n",
    "                            elif (\n",
    "                                c_min >= np.iinfo(np.int64).min\n",
    "                                and c_max <= np.iinfo(np.int64).max\n",
    "                            ):\n",
    "                                df = df.with_columns(df[col].cast(pl.Int64))\n",
    "                    elif col_type in float_types:\n",
    "                        if (\n",
    "                            c_min > np.finfo(np.float32).min\n",
    "                            and c_max < np.finfo(np.float32).max\n",
    "                        ):\n",
    "                            df = df.with_columns(df[col].cast(pl.Float32))\n",
    "\n",
    "        print(\n",
    "            f\"Memory usage of dataframe \\\"{name}\\\" became {round(df.estimated_size('mb'), 4)} MB.\"\n",
    "        )\n",
    "\n",
    "        return df\n",
    "\n",
    "    def to_pandas(df: pl.DataFrame, cat_cols: list[str] = None) -> (pd.DataFrame, list[str]):  # type: ignore\n",
    "        \"\"\"\n",
    "        Converts a Polars DataFrame to a Pandas DataFrame.\n",
    "\n",
    "        Args:\n",
    "        - df (pl.DataFrame): Polars DataFrame to convert.\n",
    "        - cat_cols (list[str]): List of categorical columns. Default is None.\n",
    "\n",
    "        Returns:\n",
    "        - (pd.DataFrame, list[str]): Tuple containing the converted Pandas DataFrame and categorical columns.\n",
    "        \"\"\"\n",
    "        df: pd.DataFrame = df.to_pandas()\n",
    "\n",
    "        if cat_cols is None:\n",
    "            cat_cols = list(df.select_dtypes(\"object\").columns)\n",
    "\n",
    "        df[cat_cols] = df[cat_cols].astype(\"str\")\n",
    "\n",
    "        return df, cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ed8907e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T15:48:50.227501Z",
     "iopub.status.busy": "2024-05-23T15:48:50.226477Z",
     "iopub.status.idle": "2024-05-23T15:48:50.231679Z",
     "shell.execute_reply": "2024-05-23T15:48:50.230541Z"
    },
    "papermill": {
     "duration": 0.015837,
     "end_time": "2024-05-23T15:48:50.234039",
     "exception": false,
     "start_time": "2024-05-23T15:48:50.218202",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# feat_defs:pl.DataFrame = Utility.find_feat_occur(TRAIN_DIR / \"train_*.parquet\", \"P\")\n",
    "# feat_defs:pl.DataFrame = Utility.find_feat_occur(TRAIN_DIR / \"train_*.parquet\", \"M\")\n",
    "# feat_defs:pl.DataFrame = Utility.find_feat_occur(TRAIN_DIR / \"train_*.parquet\", \"A\")\n",
    "# feat_defs:pl.DataFrame = Utility.find_feat_occur(TRAIN_DIR / \"train_*.parquet\", \"D\")\n",
    "# feat_defs:pl.DataFrame = Utility.find_feat_occur(TRAIN_DIR / \"train_*.parquet\", \"T\")\n",
    "# feat_defs:pl.DataFrame = Utility.find_feat_occur(TRAIN_DIR / \"train_*.parquet\", \"L\")\n",
    "# feat_defs:pl.DataFrame = pl.read_csv(ROOT / \"feature_definitions.csv\")\n",
    "# with pl.Config(fmt_str_lengths=1000, tbl_rows=-1, tbl_width_chars=180):\n",
    "#     print(feat_defs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f76317b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T15:48:50.249528Z",
     "iopub.status.busy": "2024-05-23T15:48:50.249220Z",
     "iopub.status.idle": "2024-05-23T15:48:50.266755Z",
     "shell.execute_reply": "2024-05-23T15:48:50.265674Z"
    },
    "papermill": {
     "duration": 0.027513,
     "end_time": "2024-05-23T15:48:50.268788",
     "exception": false,
     "start_time": "2024-05-23T15:48:50.241275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Aggregator:\n",
    "    @staticmethod\n",
    "    def max_expr(df: pl.LazyFrame) -> list[pl.Series]:\n",
    "        \"\"\"\n",
    "        Generates expressions for calculating maximum values for specific columns.\n",
    "\n",
    "        Args:\n",
    "        - df (pl.LazyFrame): Input LazyFrame.\n",
    "\n",
    "        Returns:\n",
    "        - list[pl.Series]: List of expressions for maximum values.\n",
    "        \"\"\"\n",
    "        cols: list[str] = [\n",
    "            col\n",
    "            for col in df.columns\n",
    "            if (col[-1] in (\"P\", \"M\", \"A\", \"D\", \"T\", \"L\")) or (\"num_group\" in col)\n",
    "        ]\n",
    "\n",
    "        expr_max: list[pl.Series] = [\n",
    "            pl.col(col).max().alias(f\"max_{col}\") for col in cols\n",
    "        ]\n",
    "\n",
    "        return expr_max\n",
    "\n",
    "    @staticmethod\n",
    "    def min_expr(df: pl.LazyFrame) -> list[pl.Series]:\n",
    "        \"\"\"\n",
    "        Generates expressions for calculating minimum values for specific columns.\n",
    "\n",
    "        Args:\n",
    "        - df (pl.LazyFrame): Input LazyFrame.\n",
    "\n",
    "        Returns:\n",
    "        - list[pl.Series]: List of expressions for minimum values.\n",
    "        \"\"\"\n",
    "        cols: list[str] = [\n",
    "            col\n",
    "            for col in df.columns\n",
    "            if (col[-1] in (\"P\", \"M\", \"A\", \"D\", \"T\", \"L\")) or (\"num_group\" in col)\n",
    "        ]\n",
    "\n",
    "        expr_min: list[pl.Series] = [\n",
    "            pl.col(col).min().alias(f\"min_{col}\") for col in cols\n",
    "        ]\n",
    "\n",
    "        return expr_min\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_expr(df: pl.LazyFrame) -> list[pl.Series]:\n",
    "        \"\"\"\n",
    "        Generates expressions for calculating mean values for specific columns.\n",
    "\n",
    "        Args:\n",
    "        - df (pl.LazyFrame): Input LazyFrame.\n",
    "\n",
    "        Returns:\n",
    "        - list[pl.Series]: List of expressions for mean values.\n",
    "        \"\"\"\n",
    "        cols: list[str] = [col for col in df.columns if col.endswith((\"P\", \"A\", \"D\"))]\n",
    "\n",
    "        expr_mean: list[pl.Series] = [\n",
    "            pl.col(col).mean().alias(f\"mean_{col}\") for col in cols\n",
    "        ]\n",
    "\n",
    "        return expr_mean\n",
    "\n",
    "    @staticmethod\n",
    "    def var_expr(df: pl.LazyFrame) -> list[pl.Series]:\n",
    "        \"\"\"\n",
    "        Generates expressions for calculating variance for specific columns.\n",
    "\n",
    "        Args:\n",
    "        - df (pl.LazyFrame): Input LazyFrame.\n",
    "\n",
    "        Returns:\n",
    "        - list[pl.Series]: List of expressions for variance.\n",
    "        \"\"\"\n",
    "        cols: list[str] = [col for col in df.columns if col.endswith((\"P\", \"A\", \"D\"))]\n",
    "\n",
    "        expr_mean: list[pl.Series] = [\n",
    "            pl.col(col).var().alias(f\"var_{col}\") for col in cols\n",
    "        ]\n",
    "\n",
    "        return expr_mean\n",
    "\n",
    "    @staticmethod\n",
    "    def mode_expr(df: pl.LazyFrame) -> list[pl.Series]:\n",
    "        \"\"\"\n",
    "        Generates expressions for calculating mode values for specific columns.\n",
    "\n",
    "        Args:\n",
    "        - df (pl.LazyFrame): Input LazyFrame.\n",
    "\n",
    "        Returns:\n",
    "        - list[pl.Series]: List of expressions for mode values.\n",
    "        \"\"\"\n",
    "        cols: list[str] = [col for col in df.columns if col.endswith(\"M\")]\n",
    "\n",
    "        expr_mode: list[pl.Series] = [\n",
    "            pl.col(col).drop_nulls().mode().first().alias(f\"mode_{col}\") for col in cols\n",
    "        ]\n",
    "\n",
    "        return expr_mode\n",
    "\n",
    "    @staticmethod\n",
    "    def get_exprs(df: pl.LazyFrame) -> list[pl.Series]:\n",
    "        \"\"\"\n",
    "        Combines expressions for maximum, mean, and variance calculations.\n",
    "\n",
    "        Args:\n",
    "        - df (pl.LazyFrame): Input LazyFrame.\n",
    "\n",
    "        Returns:\n",
    "        - list[pl.Series]: List of combined expressions.\n",
    "        \"\"\"\n",
    "        exprs = (\n",
    "            Aggregator.max_expr(df) + Aggregator.mean_expr(df) + Aggregator.var_expr(df)\n",
    "        )\n",
    "\n",
    "        return exprs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2324907e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T15:48:50.283076Z",
     "iopub.status.busy": "2024-05-23T15:48:50.282795Z",
     "iopub.status.idle": "2024-05-23T15:48:50.298543Z",
     "shell.execute_reply": "2024-05-23T15:48:50.297556Z"
    },
    "papermill": {
     "duration": 0.02559,
     "end_time": "2024-05-23T15:48:50.300679",
     "exception": false,
     "start_time": "2024-05-23T15:48:50.275089",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SchemaGen:\n",
    "    @staticmethod\n",
    "    def change_dtypes(df: pl.LazyFrame) -> pl.LazyFrame:\n",
    "        \"\"\"\n",
    "        Changes the data types of columns in the DataFrame.\n",
    "\n",
    "        Args:\n",
    "        - df (pl.LazyFrame): Input LazyFrame.\n",
    "\n",
    "        Returns:\n",
    "        - pl.LazyFrame: LazyFrame with modified data types.\n",
    "        \"\"\"\n",
    "        for col in df.columns:\n",
    "            if col == \"case_id\":\n",
    "                df = df.with_columns(pl.col(col).cast(pl.UInt32).alias(col))\n",
    "            elif col in [\"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.UInt16).alias(col))\n",
    "            elif col == \"date_decision\" or col[-1] == \"D\":\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Date).alias(col))\n",
    "            elif col[-1] in [\"P\", \"A\"]:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Float64).alias(col))\n",
    "            elif col[-1] in (\"M\",):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.String))\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def scan_files(glob_path: str, depth: int = None) -> pl.LazyFrame:\n",
    "        \"\"\"\n",
    "        Scans Parquet files matching the glob pattern and combines them into a LazyFrame.\n",
    "\n",
    "        Args:\n",
    "        - glob_path (str): Glob pattern to match Parquet files.\n",
    "        - depth (int, optional): Depth level for data aggregation. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "        - pl.LazyFrame: Combined LazyFrame.\n",
    "        \"\"\"\n",
    "        chunks: list[pl.LazyFrame] = []\n",
    "        for path in glob(str(glob_path)):\n",
    "            df: pl.LazyFrame = pl.scan_parquet(\n",
    "                path, low_memory=True, rechunk=True\n",
    "            ).pipe(SchemaGen.change_dtypes)\n",
    "            print(f\"File {Path(path).stem} loaded into memory.\")\n",
    "\n",
    "            if depth in (1, 2):\n",
    "                exprs: list[pl.Series] = Aggregator.get_exprs(df)\n",
    "                df = df.group_by(\"case_id\").agg(exprs)\n",
    "\n",
    "                del exprs\n",
    "                gc.collect()\n",
    "\n",
    "            chunks.append(df)\n",
    "\n",
    "        df = pl.concat(chunks, how=\"vertical_relaxed\")\n",
    "\n",
    "        del chunks\n",
    "        gc.collect()\n",
    "\n",
    "        df = df.unique(subset=[\"case_id\"])\n",
    "\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def join_dataframes(\n",
    "        df_base: pl.LazyFrame,\n",
    "        depth_0: list[pl.LazyFrame],\n",
    "        depth_1: list[pl.LazyFrame],\n",
    "        depth_2: list[pl.LazyFrame],\n",
    "    ) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Joins multiple LazyFrames with a base LazyFrame.\n",
    "\n",
    "        Args:\n",
    "        - df_base (pl.LazyFrame): Base LazyFrame.\n",
    "        - depth_0 (list[pl.LazyFrame]): List of LazyFrames for depth 0.\n",
    "        - depth_1 (list[pl.LazyFrame]): List of LazyFrames for depth 1.\n",
    "        - depth_2 (list[pl.LazyFrame]): List of LazyFrames for depth 2.\n",
    "\n",
    "        Returns:\n",
    "        - pl.DataFrame: Joined DataFrame.\n",
    "        \"\"\"\n",
    "        for i, df in enumerate(depth_0 + depth_1 + depth_2):\n",
    "            df_base = df_base.join(df, how=\"left\", on=\"case_id\", suffix=f\"_{i}\")\n",
    "\n",
    "        return df_base.collect().pipe(Utility.reduce_memory_usage, \"df_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37a1ae9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T15:48:50.315028Z",
     "iopub.status.busy": "2024-05-23T15:48:50.314779Z",
     "iopub.status.idle": "2024-05-23T15:48:50.330992Z",
     "shell.execute_reply": "2024-05-23T15:48:50.330176Z"
    },
    "papermill": {
     "duration": 0.025648,
     "end_time": "2024-05-23T15:48:50.332946",
     "exception": false,
     "start_time": "2024-05-23T15:48:50.307298",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_cols(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Filters columns in the DataFrame based on null percentage and unique values for string columns.\n",
    "\n",
    "    Args:\n",
    "    - df (pl.DataFrame): Input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - pl.DataFrame: DataFrame with filtered columns.\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        if col not in [\"case_id\", \"year\", \"month\", \"week_num\", \"target\"]:\n",
    "            null_pct = df[col].is_null().mean()\n",
    "\n",
    "            if null_pct > 0.95:\n",
    "                df = df.drop(col)\n",
    "\n",
    "    for col in df.columns:\n",
    "        if (col not in [\"case_id\", \"year\", \"month\", \"week_num\", \"target\"]) & (\n",
    "            df[col].dtype == pl.String\n",
    "        ):\n",
    "            freq = df[col].n_unique()\n",
    "\n",
    "            if (freq > 200) | (freq == 1):\n",
    "                df = df.drop(col)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def transform_cols(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Transforms columns in the DataFrame according to predefined rules.\n",
    "\n",
    "    Args:\n",
    "    - df (pl.DataFrame): Input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - pl.DataFrame: DataFrame with transformed columns.\n",
    "    \"\"\"\n",
    "    if \"riskassesment_302T\" in df.columns:\n",
    "        if df[\"riskassesment_302T\"].dtype == pl.Null:\n",
    "            df = df.with_columns(\n",
    "                [\n",
    "                    pl.Series(\n",
    "                        \"riskassesment_302T_rng\", df[\"riskassesment_302T\"], pl.UInt8\n",
    "                    ),\n",
    "                    pl.Series(\n",
    "                        \"riskassesment_302T_mean\", df[\"riskassesment_302T\"], pl.UInt8\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            pct_low: pl.Series = (\n",
    "                df[\"riskassesment_302T\"]\n",
    "                .str.split(\" - \")\n",
    "                .apply(lambda x: x[0].replace(\"%\", \"\"))\n",
    "                .cast(pl.UInt8)\n",
    "            )\n",
    "            pct_high: pl.Series = (\n",
    "                df[\"riskassesment_302T\"]\n",
    "                .str.split(\" - \")\n",
    "                .apply(lambda x: x[1].replace(\"%\", \"\"))\n",
    "                .cast(pl.UInt8)\n",
    "            )\n",
    "\n",
    "            diff: pl.Series = pct_high - pct_low\n",
    "            avg: pl.Series = ((pct_low + pct_high) / 2).cast(pl.Float32)\n",
    "\n",
    "            del pct_high, pct_low\n",
    "            gc.collect()\n",
    "\n",
    "            df = df.with_columns(\n",
    "                [\n",
    "                    diff.alias(\"riskassesment_302T_rng\"),\n",
    "                    avg.alias(\"riskassesment_302T_mean\"),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        df.drop(\"riskassesment_302T\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def handle_dates(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Handles date columns in the DataFrame.\n",
    "\n",
    "    Args:\n",
    "    - df (pl.DataFrame): Input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - pl.DataFrame: DataFrame with transformed date columns.\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        if col.endswith(\"D\"):\n",
    "            df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))\n",
    "            df = df.with_columns(pl.col(col).dt.total_days().cast(pl.Int32))\n",
    "\n",
    "    df = df.rename(\n",
    "        {\n",
    "            \"MONTH\": \"month\",\n",
    "            \"WEEK_NUM\": \"week_num\"\n",
    "        }\n",
    "    )\n",
    "            \n",
    "    df = df.with_columns(\n",
    "        [\n",
    "            pl.col(\"date_decision\").dt.year().alias(\"year\").cast(pl.Int16),\n",
    "            pl.col(\"date_decision\").dt.day().alias(\"day\").cast(pl.UInt8),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return df.drop(\"date_decision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f09a6b75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T15:48:50.346965Z",
     "iopub.status.busy": "2024-05-23T15:48:50.346685Z",
     "iopub.status.idle": "2024-05-23T15:51:22.106490Z",
     "shell.execute_reply": "2024-05-23T15:51:22.105391Z"
    },
    "papermill": {
     "duration": 151.76968,
     "end_time": "2024-05-23T15:51:22.108961",
     "exception": false,
     "start_time": "2024-05-23T15:48:50.339281",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File train_base loaded into memory.\n",
      "File train_static_cb_0 loaded into memory.\n",
      "File train_static_0_0 loaded into memory.\n",
      "File train_static_0_1 loaded into memory.\n",
      "File train_applprev_1_1 loaded into memory.\n",
      "File train_applprev_1_0 loaded into memory.\n",
      "File train_tax_registry_a_1 loaded into memory.\n",
      "File train_tax_registry_b_1 loaded into memory.\n",
      "File train_tax_registry_c_1 loaded into memory.\n",
      "File train_credit_bureau_a_1_3 loaded into memory.\n",
      "File train_credit_bureau_a_1_2 loaded into memory.\n",
      "File train_credit_bureau_a_1_0 loaded into memory.\n",
      "File train_credit_bureau_a_1_1 loaded into memory.\n",
      "File train_credit_bureau_b_1 loaded into memory.\n",
      "File train_other_1 loaded into memory.\n",
      "File train_person_1 loaded into memory.\n",
      "File train_deposit_1 loaded into memory.\n",
      "File train_debitcard_1 loaded into memory.\n",
      "File train_credit_bureau_a_2_6 loaded into memory.\n",
      "File train_credit_bureau_a_2_1 loaded into memory.\n",
      "File train_credit_bureau_a_2_0 loaded into memory.\n",
      "File train_credit_bureau_a_2_7 loaded into memory.\n",
      "File train_credit_bureau_a_2_5 loaded into memory.\n",
      "File train_credit_bureau_a_2_2 loaded into memory.\n",
      "File train_credit_bureau_a_2_4 loaded into memory.\n",
      "File train_credit_bureau_a_2_9 loaded into memory.\n",
      "File train_credit_bureau_a_2_3 loaded into memory.\n",
      "File train_credit_bureau_a_2_10 loaded into memory.\n",
      "File train_credit_bureau_a_2_8 loaded into memory.\n",
      "File train_credit_bureau_b_2 loaded into memory.\n",
      "Memory usage of dataframe \"df_train\" is 6783.1317 MB.\n",
      "Memory usage of dataframe \"df_train\" became 4174.0953 MB.\n",
      "Memory usage of dataframe \"df_train\" is 2870.9171 MB.\n",
      "Memory usage of dataframe \"df_train\" became 2665.6302 MB.\n",
      "Train data shape: (1526659, 472)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 472)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>case_id</th><th>month</th><th>week_num</th><th>target</th><th>assignmentdate_238D</th><th>assignmentdate_4527235D</th><th>birthdate_574D</th><th>contractssum_5085716L</th><th>dateofbirth_337D</th><th>days120_123L</th><th>days180_256L</th><th>days30_165L</th><th>days360_512L</th><th>days90_310L</th><th>description_5085714M</th><th>education_1103M</th><th>education_88M</th><th>firstquarter_103L</th><th>fourthquarter_440L</th><th>maritalst_385M</th><th>maritalst_893M</th><th>numberofqueries_373L</th><th>pmtaverage_3A</th><th>pmtaverage_4527227A</th><th>pmtcount_4527229L</th><th>pmtcount_693L</th><th>pmtscount_423L</th><th>pmtssum_45A</th><th>requesttype_4525192L</th><th>responsedate_1012D</th><th>responsedate_4527233D</th><th>responsedate_4917613D</th><th>secondquarter_766L</th><th>thirdquarter_1082L</th><th>actualdpdtolerance_344P</th><th>amtinstpaidbefduel24m_4187115A</th><th>annuity_780A</th><th>&hellip;</th><th>mean_mainoccupationinc_384A</th><th>max_amount_416A</th><th>max_num_group1_10</th><th>max_openingdate_313D</th><th>mean_amount_416A</th><th>mean_openingdate_313D</th><th>max_num_group1_11</th><th>max_openingdate_857D</th><th>mean_openingdate_857D</th><th>max_collater_typofvalofguarant_298M</th><th>max_collater_typofvalofguarant_407M</th><th>max_collater_valueofguarantee_1124L</th><th>max_collater_valueofguarantee_876L</th><th>max_collaterals_typeofguarante_359M</th><th>max_collaterals_typeofguarante_669M</th><th>max_num_group1_12</th><th>max_num_group2</th><th>max_pmts_dpd_1073P</th><th>max_pmts_dpd_303P</th><th>max_pmts_month_158T</th><th>max_pmts_month_706T</th><th>max_pmts_overdue_1140A</th><th>max_pmts_overdue_1152A</th><th>max_pmts_year_1139T</th><th>max_pmts_year_507T</th><th>max_subjectroles_name_541M</th><th>max_subjectroles_name_838M</th><th>mean_pmts_dpd_1073P</th><th>mean_pmts_dpd_303P</th><th>mean_pmts_overdue_1140A</th><th>mean_pmts_overdue_1152A</th><th>var_pmts_dpd_1073P</th><th>var_pmts_dpd_303P</th><th>var_pmts_overdue_1140A</th><th>var_pmts_overdue_1152A</th><th>year</th><th>day</th></tr><tr><td>u32</td><td>u32</td><td>u8</td><td>u8</td><td>i16</td><td>u8</td><td>i16</td><td>f32</td><td>i32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>str</td><td>str</td><td>str</td><td>f32</td><td>f32</td><td>str</td><td>str</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>str</td><td>i8</td><td>u8</td><td>i8</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>&hellip;</td><td>f32</td><td>f32</td><td>u8</td><td>i16</td><td>f32</td><td>i16</td><td>u8</td><td>i16</td><td>i16</td><td>str</td><td>str</td><td>f32</td><td>f32</td><td>str</td><td>str</td><td>u16</td><td>u8</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>str</td><td>str</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>u16</td><td>u8</td></tr></thead><tbody><tr><td>129911</td><td>201905</td><td>19</td><td>0</td><td>null</td><td>null</td><td>-14202</td><td>null</td><td>-14202</td><td>0.0</td><td>0.0</td><td>0.0</td><td>2.0</td><td>0.0</td><td>&quot;a55475b1&quot;</td><td>&quot;717ddd49&quot;</td><td>&quot;a55475b1&quot;</td><td>1.0</td><td>0.0</td><td>&quot;ecd83604&quot;</td><td>&quot;a55475b1&quot;</td><td>2.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>6.0</td><td>8550.0</td><td>null</td><td>14</td><td>null</td><td>null</td><td>1.0</td><td>1.0</td><td>0.0</td><td>51314.941406</td><td>6121.0</td><td>&hellip;</td><td>140000.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>0.0</td><td>null</td><td>&quot;a55475b1&quot;</td><td>&quot;c7a5ad39&quot;</td><td>0</td><td>23</td><td>0.0</td><td>null</td><td>12.0</td><td>null</td><td>0.0</td><td>null</td><td>2020.0</td><td>null</td><td>&quot;a55475b1&quot;</td><td>&quot;ab3c25cf&quot;</td><td>0.0</td><td>null</td><td>0.0</td><td>null</td><td>0.0</td><td>null</td><td>0.0</td><td>null</td><td>2019</td><td>20</td></tr><tr><td>1557193</td><td>201909</td><td>38</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>-17437</td><td>13.0</td><td>18.0</td><td>0.0</td><td>24.0</td><td>7.0</td><td>&quot;a55475b1&quot;</td><td>&quot;6b2ae0fa&quot;</td><td>&quot;a55475b1&quot;</td><td>4.0</td><td>2.0</td><td>&quot;a7fcb6e5&quot;</td><td>&quot;a55475b1&quot;</td><td>24.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>10.0</td><td>24559.800781</td><td>&quot;DEDUCTION_6&quot;</td><td>14</td><td>14</td><td>null</td><td>13.0</td><td>10.0</td><td>0.0</td><td>186394.609375</td><td>1726.200073</td><td>&hellip;</td><td>100000.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>0.0</td><td>0.0</td><td>&quot;c7a5ad39&quot;</td><td>&quot;c7a5ad39&quot;</td><td>7</td><td>35</td><td>1.0</td><td>21.0</td><td>12.0</td><td>12.0</td><td>6161.399902</td><td>118137.148438</td><td>2020.0</td><td>2020.0</td><td>&quot;ab3c25cf&quot;</td><td>&quot;ab3c25cf&quot;</td><td>0.015385</td><td>0.727273</td><td>94.790771</td><td>2902.225342</td><td>0.015385</td><td>8.65</td><td>584043.875</td><td>2.37010464e8</td><td>2019</td><td>28</td></tr><tr><td>2698086</td><td>202009</td><td>88</td><td>0</td><td>null</td><td>null</td><td>null</td><td>56496.730469</td><td>-20283</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>&quot;2fc785b2&quot;</td><td>&quot;6b2ae0fa&quot;</td><td>&quot;a55475b1&quot;</td><td>4.0</td><td>0.0</td><td>&quot;3439d993&quot;</td><td>&quot;a55475b1&quot;</td><td>0.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>14</td><td>0.0</td><td>3.0</td><td>0.0</td><td>146758.40625</td><td>12445.0</td><td>&hellip;</td><td>60000.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>6.972378e6</td><td>6.972378e6</td><td>&quot;c7a5ad39&quot;</td><td>&quot;c7a5ad39&quot;</td><td>7</td><td>35</td><td>0.0</td><td>6.0</td><td>12.0</td><td>12.0</td><td>0.0</td><td>4670.800293</td><td>2021.0</td><td>2020.0</td><td>&quot;ab3c25cf&quot;</td><td>&quot;ab3c25cf&quot;</td><td>0.0</td><td>0.147287</td><td>0.0</td><td>214.924622</td><td>0.0</td><td>0.48595</td><td>0.0</td><td>642761.8125</td><td>2020</td><td>11</td></tr><tr><td>601817</td><td>201901</td><td>0</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>1075.800049</td><td>&hellip;</td><td>36000.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>2019</td><td>4</td></tr><tr><td>1581484</td><td>201910</td><td>40</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>-13070</td><td>6.0</td><td>7.0</td><td>3.0</td><td>9.0</td><td>5.0</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>2.0</td><td>3.0</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>9.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;DEDUCTION_6&quot;</td><td>null</td><td>14</td><td>null</td><td>3.0</td><td>2.0</td><td>0.0</td><td>48511.601562</td><td>4165.0</td><td>&hellip;</td><td>50000.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>0.0</td><td>0.0</td><td>&quot;c7a5ad39&quot;</td><td>&quot;c7a5ad39&quot;</td><td>4</td><td>23</td><td>54.0</td><td>11.0</td><td>12.0</td><td>12.0</td><td>7088.065918</td><td>1418.890015</td><td>2020.0</td><td>2020.0</td><td>&quot;ab3c25cf&quot;</td><td>&quot;ab3c25cf&quot;</td><td>3.6875</td><td>0.423077</td><td>715.639282</td><td>54.572693</td><td>115.189514</td><td>4.653846</td><td>2.8907e6</td><td>77432.648438</td><td>2019</td><td>14</td></tr><tr><td>906581</td><td>201912</td><td>50</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>-16912</td><td>1.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>1.0</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>0.0</td><td>1.0</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>1.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;DEDUCTION_6&quot;</td><td>null</td><td>14</td><td>null</td><td>0.0</td><td>0.0</td><td>null</td><td>null</td><td>4000.0</td><td>&hellip;</td><td>50000.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>0.0</td><td>100000.0</td><td>&quot;a55475b1&quot;</td><td>&quot;c7a5ad39&quot;</td><td>3</td><td>23</td><td>0.0</td><td>0.0</td><td>12.0</td><td>12.0</td><td>0.0</td><td>0.0</td><td>2020.0</td><td>2009.0</td><td>&quot;ab3c25cf&quot;</td><td>&quot;ab3c25cf&quot;</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>2019</td><td>21</td></tr><tr><td>1357232</td><td>201905</td><td>17</td><td>0</td><td>null</td><td>null</td><td>-21000</td><td>null</td><td>-21000</td><td>0.0</td><td>3.0</td><td>0.0</td><td>5.0</td><td>0.0</td><td>&quot;a55475b1&quot;</td><td>&quot;717ddd49&quot;</td><td>&quot;a55475b1&quot;</td><td>2.0</td><td>5.0</td><td>&quot;b6cabe76&quot;</td><td>&quot;a55475b1&quot;</td><td>5.0</td><td>0.0</td><td>null</td><td>null</td><td>0.0</td><td>null</td><td>null</td><td>null</td><td>14</td><td>null</td><td>null</td><td>3.0</td><td>1.0</td><td>0.0</td><td>11674.0</td><td>3201.800049</td><td>&hellip;</td><td>50000.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>0.0</td><td>null</td><td>&quot;a55475b1&quot;</td><td>&quot;c7a5ad39&quot;</td><td>1</td><td>23</td><td>0.0</td><td>null</td><td>12.0</td><td>null</td><td>0.0</td><td>null</td><td>2020.0</td><td>null</td><td>&quot;a55475b1&quot;</td><td>&quot;ab3c25cf&quot;</td><td>0.0</td><td>null</td><td>0.0</td><td>null</td><td>0.0</td><td>null</td><td>0.0</td><td>null</td><td>2019</td><td>1</td></tr><tr><td>1830799</td><td>202003</td><td>64</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>-12751</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>0.0</td><td>0.0</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>0.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;DEDUCTION_6&quot;</td><td>null</td><td>14</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>3139.199951</td><td>4473.600098</td><td>&hellip;</td><td>70000.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>0.0</td><td>null</td><td>&quot;a55475b1&quot;</td><td>&quot;c7a5ad39&quot;</td><td>0</td><td>35</td><td>39.0</td><td>null</td><td>12.0</td><td>null</td><td>2.4</td><td>null</td><td>2021.0</td><td>null</td><td>&quot;a55475b1&quot;</td><td>&quot;ab3c25cf&quot;</td><td>2.777778</td><td>null</td><td>0.266667</td><td>null</td><td>88.418304</td><td>null</td><td>0.602353</td><td>null</td><td>2020</td><td>29</td></tr><tr><td>868832</td><td>201911</td><td>46</td><td>0</td><td>null</td><td>14</td><td>null</td><td>null</td><td>-22238</td><td>0.0</td><td>0.0</td><td>0.0</td><td>2.0</td><td>0.0</td><td>&quot;a55475b1&quot;</td><td>&quot;39a0853f&quot;</td><td>&quot;a55475b1&quot;</td><td>2.0</td><td>1.0</td><td>&quot;b6cabe76&quot;</td><td>&quot;a55475b1&quot;</td><td>2.0</td><td>null</td><td>2979.400146</td><td>6.0</td><td>null</td><td>null</td><td>null</td><td>&quot;PENSION_6&quot;</td><td>null</td><td>14</td><td>null</td><td>1.0</td><td>3.0</td><td>null</td><td>null</td><td>2776.400146</td><td>&hellip;</td><td>27000.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>0.0</td><td>0.0</td><td>&quot;c7a5ad39&quot;</td><td>&quot;c7a5ad39&quot;</td><td>1</td><td>35</td><td>4.0</td><td>76.0</td><td>12.0</td><td>12.0</td><td>2044.501953</td><td>2899.199951</td><td>2020.0</td><td>2020.0</td><td>&quot;ab3c25cf&quot;</td><td>&quot;ab3c25cf&quot;</td><td>0.625</td><td>29.333334</td><td>362.272827</td><td>2183.855957</td><td>1.63587</td><td>941.06665</td><td>563574.5625</td><td>1.3017e6</td><td>2019</td><td>20</td></tr><tr><td>1433954</td><td>201907</td><td>26</td><td>0</td><td>null</td><td>null</td><td>-18114</td><td>null</td><td>-18114</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>&quot;a55475b1&quot;</td><td>&quot;6b2ae0fa&quot;</td><td>&quot;a55475b1&quot;</td><td>0.0</td><td>1.0</td><td>&quot;3439d993&quot;</td><td>&quot;a55475b1&quot;</td><td>1.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.0</td><td>0.0</td><td>null</td><td>14</td><td>null</td><td>null</td><td>2.0</td><td>3.0</td><td>0.0</td><td>16403.599609</td><td>1439.400024</td><td>&hellip;</td><td>72000.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>0.0</td><td>0.0</td><td>&quot;c7a5ad39&quot;</td><td>&quot;c7a5ad39&quot;</td><td>5</td><td>35</td><td>0.0</td><td>24.0</td><td>12.0</td><td>12.0</td><td>0.0</td><td>9347.862305</td><td>2020.0</td><td>2020.0</td><td>&quot;ab3c25cf&quot;</td><td>&quot;ab3c25cf&quot;</td><td>0.0</td><td>1.045455</td><td>0.0</td><td>373.042511</td><td>0.0</td><td>16.788584</td><td>0.0</td><td>2.275334e6</td><td>2019</td><td>6</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 472)\n",
       "┌─────────┬────────┬──────────┬────────┬───┬─────────────────────┬────────────────────┬──────┬─────┐\n",
       "│ case_id ┆ month  ┆ week_num ┆ target ┆ … ┆ var_pmts_overdue_11 ┆ var_pmts_overdue_1 ┆ year ┆ day │\n",
       "│ ---     ┆ ---    ┆ ---      ┆ ---    ┆   ┆ 40A                 ┆ 152A               ┆ ---  ┆ --- │\n",
       "│ u32     ┆ u32    ┆ u8       ┆ u8     ┆   ┆ ---                 ┆ ---                ┆ u16  ┆ u8  │\n",
       "│         ┆        ┆          ┆        ┆   ┆ f32                 ┆ f32                ┆      ┆     │\n",
       "╞═════════╪════════╪══════════╪════════╪═══╪═════════════════════╪════════════════════╪══════╪═════╡\n",
       "│ 129911  ┆ 201905 ┆ 19       ┆ 0      ┆ … ┆ 0.0                 ┆ null               ┆ 2019 ┆ 20  │\n",
       "│ 1557193 ┆ 201909 ┆ 38       ┆ 0      ┆ … ┆ 584043.875          ┆ 2.37010464e8       ┆ 2019 ┆ 28  │\n",
       "│ 2698086 ┆ 202009 ┆ 88       ┆ 0      ┆ … ┆ 0.0                 ┆ 642761.8125        ┆ 2020 ┆ 11  │\n",
       "│ 601817  ┆ 201901 ┆ 0        ┆ 0      ┆ … ┆ null                ┆ null               ┆ 2019 ┆ 4   │\n",
       "│ 1581484 ┆ 201910 ┆ 40       ┆ 0      ┆ … ┆ 2.8907e6            ┆ 77432.648438       ┆ 2019 ┆ 14  │\n",
       "│ 906581  ┆ 201912 ┆ 50       ┆ 0      ┆ … ┆ 0.0                 ┆ 0.0                ┆ 2019 ┆ 21  │\n",
       "│ 1357232 ┆ 201905 ┆ 17       ┆ 0      ┆ … ┆ 0.0                 ┆ null               ┆ 2019 ┆ 1   │\n",
       "│ 1830799 ┆ 202003 ┆ 64       ┆ 0      ┆ … ┆ 0.602353            ┆ null               ┆ 2020 ┆ 29  │\n",
       "│ 868832  ┆ 201911 ┆ 46       ┆ 0      ┆ … ┆ 563574.5625         ┆ 1.3017e6           ┆ 2019 ┆ 20  │\n",
       "│ 1433954 ┆ 201907 ┆ 26       ┆ 0      ┆ … ┆ 0.0                 ┆ 2.275334e6         ┆ 2019 ┆ 6   │\n",
       "└─────────┴────────┴──────────┴────────┴───┴─────────────────────┴────────────────────┴──────┴─────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_store: dict = {\n",
    "    \"df_base\": SchemaGen.scan_files(TRAIN_DIR / \"train_base.parquet\"),\n",
    "    \"depth_0\": [\n",
    "        SchemaGen.scan_files(TRAIN_DIR / \"train_static_cb_0.parquet\"),\n",
    "        SchemaGen.scan_files(TRAIN_DIR / \"train_static_0_*.parquet\"),\n",
    "    ],\n",
    "    \"depth_1\": [\n",
    "        SchemaGen.scan_files(TRAIN_DIR / \"train_applprev_1_*.parquet\", 1),\n",
    "        SchemaGen.scan_files(TRAIN_DIR / \"train_tax_registry_a_1.parquet\", 1),\n",
    "        SchemaGen.scan_files(TRAIN_DIR / \"train_tax_registry_b_1.parquet\", 1),\n",
    "        SchemaGen.scan_files(TRAIN_DIR / \"train_tax_registry_c_1.parquet\", 1),\n",
    "        SchemaGen.scan_files(TRAIN_DIR / \"train_credit_bureau_a_1_*.parquet\", 1),\n",
    "        SchemaGen.scan_files(TRAIN_DIR / \"train_credit_bureau_b_1.parquet\", 1),\n",
    "        SchemaGen.scan_files(TRAIN_DIR / \"train_other_1.parquet\", 1),\n",
    "        SchemaGen.scan_files(TRAIN_DIR / \"train_person_1.parquet\", 1),\n",
    "        SchemaGen.scan_files(TRAIN_DIR / \"train_deposit_1.parquet\", 1),\n",
    "        SchemaGen.scan_files(TRAIN_DIR / \"train_debitcard_1.parquet\", 1),\n",
    "    ],\n",
    "    \"depth_2\": [\n",
    "        SchemaGen.scan_files(TRAIN_DIR / \"train_credit_bureau_a_2_*.parquet\", 2),\n",
    "        SchemaGen.scan_files(TRAIN_DIR / \"train_credit_bureau_b_2.parquet\", 2),\n",
    "    ],\n",
    "}\n",
    "\n",
    "df_train: pl.LazyFrame = (\n",
    "    SchemaGen.join_dataframes(**data_store)\n",
    "    .pipe(filter_cols)\n",
    "    .pipe(transform_cols)\n",
    "    .pipe(handle_dates)\n",
    "    .pipe(Utility.reduce_memory_usage, \"df_train\")\n",
    ")\n",
    "\n",
    "del data_store\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Train data shape: {df_train.shape}\")\n",
    "display(df_train.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48f4ce07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T15:51:22.127722Z",
     "iopub.status.busy": "2024-05-23T15:51:22.127412Z",
     "iopub.status.idle": "2024-05-23T15:51:27.077337Z",
     "shell.execute_reply": "2024-05-23T15:51:27.076193Z"
    },
    "papermill": {
     "duration": 4.961636,
     "end_time": "2024-05-23T15:51:27.079518",
     "exception": false,
     "start_time": "2024-05-23T15:51:22.117882",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File test_base loaded into memory.\n",
      "File test_static_cb_0 loaded into memory.\n",
      "File test_static_0_0 loaded into memory.\n",
      "File test_static_0_2 loaded into memory.\n",
      "File test_static_0_1 loaded into memory.\n",
      "File test_applprev_1_2 loaded into memory.\n",
      "File test_applprev_1_0 loaded into memory.\n",
      "File test_applprev_1_1 loaded into memory.\n",
      "File test_tax_registry_a_1 loaded into memory.\n",
      "File test_tax_registry_b_1 loaded into memory.\n",
      "File test_tax_registry_c_1 loaded into memory.\n",
      "File test_credit_bureau_a_1_3 loaded into memory.\n",
      "File test_credit_bureau_a_1_2 loaded into memory.\n",
      "File test_credit_bureau_a_1_1 loaded into memory.\n",
      "File test_credit_bureau_a_1_4 loaded into memory.\n",
      "File test_credit_bureau_a_1_0 loaded into memory.\n",
      "File test_credit_bureau_b_1 loaded into memory.\n",
      "File test_other_1 loaded into memory.\n",
      "File test_person_1 loaded into memory.\n",
      "File test_deposit_1 loaded into memory.\n",
      "File test_debitcard_1 loaded into memory.\n",
      "File test_credit_bureau_a_2_3 loaded into memory.\n",
      "File test_credit_bureau_a_2_9 loaded into memory.\n",
      "File test_credit_bureau_a_2_2 loaded into memory.\n",
      "File test_credit_bureau_a_2_11 loaded into memory.\n",
      "File test_credit_bureau_a_2_1 loaded into memory.\n",
      "File test_credit_bureau_a_2_6 loaded into memory.\n",
      "File test_credit_bureau_a_2_5 loaded into memory.\n",
      "File test_credit_bureau_a_2_0 loaded into memory.\n",
      "File test_credit_bureau_a_2_7 loaded into memory.\n",
      "File test_credit_bureau_a_2_10 loaded into memory.\n",
      "File test_credit_bureau_a_2_8 loaded into memory.\n",
      "File test_credit_bureau_a_2_4 loaded into memory.\n",
      "File test_credit_bureau_b_2 loaded into memory.\n",
      "Memory usage of dataframe \"df_train\" is 0.0432 MB.\n",
      "Memory usage of dataframe \"df_train\" became 0.0311 MB.\n",
      "Memory usage of dataframe \"df_test\" is 0.0184 MB.\n",
      "Memory usage of dataframe \"df_test\" became 0.0172 MB.\n",
      "Test data shape: (10, 471)\n"
     ]
    }
   ],
   "source": [
    "data_store: dict = {\n",
    "    \"df_base\": SchemaGen.scan_files(TEST_DIR / \"test_base.parquet\"),\n",
    "    \"depth_0\": [\n",
    "        SchemaGen.scan_files(TEST_DIR / \"test_static_cb_0.parquet\"),\n",
    "        SchemaGen.scan_files(TEST_DIR / \"test_static_0_*.parquet\"),\n",
    "    ],\n",
    "    \"depth_1\": [\n",
    "        SchemaGen.scan_files(TEST_DIR / \"test_applprev_1_*.parquet\", 1),\n",
    "        SchemaGen.scan_files(TEST_DIR / \"test_tax_registry_a_1.parquet\", 1),\n",
    "        SchemaGen.scan_files(TEST_DIR / \"test_tax_registry_b_1.parquet\", 1),\n",
    "        SchemaGen.scan_files(TEST_DIR / \"test_tax_registry_c_1.parquet\", 1),\n",
    "        SchemaGen.scan_files(TEST_DIR / \"test_credit_bureau_a_1_*.parquet\", 1),\n",
    "        SchemaGen.scan_files(TEST_DIR / \"test_credit_bureau_b_1.parquet\", 1),\n",
    "        SchemaGen.scan_files(TEST_DIR / \"test_other_1.parquet\", 1),\n",
    "        SchemaGen.scan_files(TEST_DIR / \"test_person_1.parquet\", 1),\n",
    "        SchemaGen.scan_files(TEST_DIR / \"test_deposit_1.parquet\", 1),\n",
    "        SchemaGen.scan_files(TEST_DIR / \"test_debitcard_1.parquet\", 1),\n",
    "    ],\n",
    "    \"depth_2\": [\n",
    "        SchemaGen.scan_files(TEST_DIR / \"test_credit_bureau_a_2_*.parquet\", 2),\n",
    "        SchemaGen.scan_files(TEST_DIR / \"test_credit_bureau_b_2.parquet\", 2),\n",
    "    ],\n",
    "}\n",
    "\n",
    "df_test: pl.DataFrame = (\n",
    "    SchemaGen.join_dataframes(**data_store)\n",
    "    .pipe(transform_cols)\n",
    "    .pipe(handle_dates)\n",
    "    .select([col for col in df_train.columns if col != \"target\"])\n",
    "    .pipe(Utility.reduce_memory_usage, \"df_test\")\n",
    ")\n",
    "\n",
    "del data_store\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Test data shape: {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c637ed17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T15:51:27.100473Z",
     "iopub.status.busy": "2024-05-23T15:51:27.100112Z",
     "iopub.status.idle": "2024-05-23T15:51:51.593537Z",
     "shell.execute_reply": "2024-05-23T15:51:51.592623Z"
    },
    "papermill": {
     "duration": 24.506756,
     "end_time": "2024-05-23T15:51:51.596128",
     "exception": false,
     "start_time": "2024-05-23T15:51:27.089372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train, cat_cols = Utility.to_pandas(df_train)\n",
    "df_test, cat_cols = Utility.to_pandas(df_test, cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbdf0284",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T15:51:51.620922Z",
     "iopub.status.busy": "2024-05-23T15:51:51.620273Z",
     "iopub.status.idle": "2024-05-23T15:51:51.629840Z",
     "shell.execute_reply": "2024-05-23T15:51:51.628850Z"
    },
    "papermill": {
     "duration": 0.024512,
     "end_time": "2024-05-23T15:51:51.631951",
     "exception": false,
     "start_time": "2024-05-23T15:51:51.607439",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VotingModel(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    A voting ensemble model that combines predictions from multiple estimators.\n",
    "\n",
    "    Parameters:\n",
    "    - estimators (list): List of base estimators.\n",
    "\n",
    "    Attributes:\n",
    "    - estimators (list): List of base estimators.\n",
    "\n",
    "    Methods:\n",
    "    - fit(X, y=None): Fit the model to the training data.\n",
    "    - predict(X): Predict class labels for samples.\n",
    "    - predict_proba(X): Predict class probabilities for samples.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, estimators: list[BaseEstimator]):\n",
    "        \"\"\"\n",
    "        Initialize the VotingModel with a list of base estimators.\n",
    "\n",
    "        Args:\n",
    "        - estimators (list): List of base estimators.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.estimators = estimators\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit the model to the training data.\n",
    "\n",
    "        Args:\n",
    "        - X: Input features.\n",
    "        - y: Target labels (ignored).\n",
    "\n",
    "        Returns:\n",
    "        - self: Returns the instance itself.\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels for samples.\n",
    "\n",
    "        Args:\n",
    "        - X: Input features.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: Predicted class labels.\n",
    "        \"\"\"\n",
    "        y_preds = [estimator.predict(X) for estimator in self.estimators]\n",
    "        return np.mean(y_preds, axis=0)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predict class probabilities for samples.\n",
    "\n",
    "        Args:\n",
    "        - X: Input features.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: Predicted class probabilities.\n",
    "        \"\"\"\n",
    "        y_preds = [estimator.predict_proba(X) for estimator in self.estimators]\n",
    "        return np.mean(y_preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "604a9a9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T15:51:51.654607Z",
     "iopub.status.busy": "2024-05-23T15:51:51.654324Z",
     "iopub.status.idle": "2024-05-23T15:51:51.671331Z",
     "shell.execute_reply": "2024-05-23T15:51:51.670201Z"
    },
    "papermill": {
     "duration": 0.030814,
     "end_time": "2024-05-23T15:51:51.673532",
     "exception": false,
     "start_time": "2024-05-23T15:51:51.642718",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "df_subm: pd.DataFrame = pd.read_csv(ROOT / \"sample_submission.csv\")\n",
    "df_subm = df_subm.set_index(\"case_id\")\n",
    "\n",
    "device: str = \"gpu\"\n",
    "est_cnt: int = 6000\n",
    "\n",
    "DRY_RUN = True if df_subm.shape[0] == 10 else False\n",
    "if DRY_RUN:\n",
    "    device = \"cpu\"\n",
    "    df_train = df_train.iloc[:50000]\n",
    "    est_cnt: int = 600\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "801c8296",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T15:51:51.695721Z",
     "iopub.status.busy": "2024-05-23T15:51:51.695367Z",
     "iopub.status.idle": "2024-05-23T15:59:45.594204Z",
     "shell.execute_reply": "2024-05-23T15:59:45.593110Z"
    },
    "papermill": {
     "duration": 473.912377,
     "end_time": "2024-05-23T15:59:45.596659",
     "exception": false,
     "start_time": "2024-05-23T15:51:51.684282",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because AUC is/are not implemented for GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.6523426\tbest: 0.6523426 (0)\ttotal: 8.74s\tremaining: 1h 27m 16s\n",
      "300:\ttest: 0.8105046\tbest: 0.8109131 (290)\ttotal: 33.7s\tremaining: 33.5s\n",
      "599:\ttest: 0.8247363\tbest: 0.8247363 (599)\ttotal: 59.1s\tremaining: 0us\n",
      "bestTest = 0.8247362971\n",
      "bestIteration = 599\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's auc: 0.829662\n",
      "[200]\tvalid_0's auc: 0.830824\n",
      "Early stopping, best iteration is:\n",
      "[143]\tvalid_0's auc: 0.835034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because AUC is/are not implemented for GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.6714274\tbest: 0.6714274 (0)\ttotal: 89.7ms\tremaining: 53.7s\n",
      "300:\ttest: 0.8027416\tbest: 0.8027416 (300)\ttotal: 25.2s\tremaining: 25s\n",
      "599:\ttest: 0.8127563\tbest: 0.8127563 (599)\ttotal: 50.5s\tremaining: 0us\n",
      "bestTest = 0.8127563\n",
      "bestIteration = 599\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's auc: 0.805648\n",
      "[200]\tvalid_0's auc: 0.817569\n",
      "[300]\tvalid_0's auc: 0.819391\n",
      "[400]\tvalid_0's auc: 0.81903\n",
      "Early stopping, best iteration is:\n",
      "[330]\tvalid_0's auc: 0.820259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because AUC is/are not implemented for GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.6229261\tbest: 0.6229261 (0)\ttotal: 88.6ms\tremaining: 53.1s\n",
      "300:\ttest: 0.8124561\tbest: 0.8133013 (285)\ttotal: 25.4s\tremaining: 25.2s\n",
      "599:\ttest: 0.8162804\tbest: 0.8164205 (505)\ttotal: 50.5s\tremaining: 0us\n",
      "bestTest = 0.8164204955\n",
      "bestIteration = 505\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's auc: 0.817325\n",
      "[200]\tvalid_0's auc: 0.815631\n",
      "Early stopping, best iteration is:\n",
      "[122]\tvalid_0's auc: 0.818803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because AUC is/are not implemented for GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.6357726\tbest: 0.6357726 (0)\ttotal: 89.9ms\tremaining: 53.9s\n",
      "300:\ttest: 0.7958853\tbest: 0.7958853 (300)\ttotal: 25.3s\tremaining: 25.2s\n",
      "599:\ttest: 0.8078007\tbest: 0.8078118 (595)\ttotal: 51s\tremaining: 0us\n",
      "bestTest = 0.8078118265\n",
      "bestIteration = 595\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's auc: 0.807054\n",
      "[200]\tvalid_0's auc: 0.817422\n",
      "[300]\tvalid_0's auc: 0.820892\n",
      "[400]\tvalid_0's auc: 0.819646\n",
      "Early stopping, best iteration is:\n",
      "[309]\tvalid_0's auc: 0.82127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because AUC is/are not implemented for GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.6762061\tbest: 0.6762061 (0)\ttotal: 91.4ms\tremaining: 54.7s\n",
      "300:\ttest: 0.8176997\tbest: 0.8176997 (300)\ttotal: 25.4s\tremaining: 25.2s\n",
      "599:\ttest: 0.8229214\tbest: 0.8229214 (599)\ttotal: 50.6s\tremaining: 0us\n",
      "bestTest = 0.8229213953\n",
      "bestIteration = 599\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's auc: 0.816239\n",
      "[200]\tvalid_0's auc: 0.815884\n",
      "Early stopping, best iteration is:\n",
      "[125]\tvalid_0's auc: 0.819562\n",
      "\n",
      "CV AUC scores for CatBoost: [0.824736277794202, 0.8127566202967934, 0.8162803991541104, 0.8078009162049481, 0.8229210803006765]\n",
      "Maximum CV AUC score for Catboost: 0.824736277794202\n",
      "\n",
      "CV AUC scores for LGBM: [0.8350344885839586, 0.8202585332993244, 0.8188025376685171, 0.8212700982986486, 0.8195618536478194]\n",
      "Maximum CV AUC score for LGBM: 0.8350344885839586\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df_train.drop(columns=[\"target\", \"case_id\", \"week_num\"])\n",
    "y = df_train[\"target\"]\n",
    "\n",
    "weeks = df_train[\"week_num\"]\n",
    "\n",
    "del df_train\n",
    "gc.collect()\n",
    "\n",
    "cv = StratifiedGroupKFold(n_splits=5, shuffle=False)\n",
    "\n",
    "params1 = {\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"colsample_bynode\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"device\": device,\n",
    "    \"extra_trees\": True,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"l1_regularization\": 0.1,\n",
    "    \"l2_regularization\": 10,\n",
    "    \"max_depth\": 20,\n",
    "    \"metric\": \"auc\",\n",
    "    \"n_estimators\": 2000,\n",
    "    \"num_leaves\": 64,\n",
    "    \"objective\": \"binary\",\n",
    "    \"random_state\": 42,\n",
    "    \"verbose\": -1,\n",
    "}\n",
    "\n",
    "params2 = {\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"colsample_bynode\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"device\": device,\n",
    "    \"extra_trees\": True,\n",
    "    \"learning_rate\": 0.03,\n",
    "    \"l1_regularization\": 0.1,\n",
    "    \"l2_regularization\": 10,\n",
    "    \"max_depth\": 16,\n",
    "    \"metric\": \"auc\",\n",
    "    \"n_estimators\": 2000,\n",
    "    \"num_leaves\": 54,\n",
    "    \"objective\": \"binary\",\n",
    "    \"random_state\": 42,\n",
    "    \"verbose\": -1,\n",
    "}\n",
    "\n",
    "fitted_models_cat = []\n",
    "fitted_models_lgb = []\n",
    "\n",
    "cv_scores_cat = []\n",
    "cv_scores_lgb = []\n",
    "\n",
    "iter_cnt = 0\n",
    "for idx_train, idx_valid in cv.split(X, y, groups=weeks):\n",
    "    X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n",
    "    X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n",
    "\n",
    "    train_pool = Pool(X_train, y_train, cat_features=cat_cols)\n",
    "    val_pool = Pool(X_valid, y_valid, cat_features=cat_cols)\n",
    "\n",
    "    clf = CatBoostClassifier(\n",
    "        best_model_min_trees = 1000,\n",
    "        boosting_type = \"Plain\",\n",
    "        eval_metric = \"AUC\",\n",
    "        iterations = est_cnt,\n",
    "        learning_rate = 0.05,\n",
    "        l2_leaf_reg = 10,\n",
    "        max_leaves = 64,\n",
    "        random_seed = 42,\n",
    "        task_type = \"GPU\",\n",
    "        use_best_model = True\n",
    "    )\n",
    "\n",
    "    clf.fit(train_pool, eval_set=val_pool, verbose=300)\n",
    "    fitted_models_cat.append(clf)\n",
    "\n",
    "    y_pred_valid = clf.predict_proba(X_valid)[:, 1]\n",
    "    auc_score = roc_auc_score(y_valid, y_pred_valid)\n",
    "    cv_scores_cat.append(auc_score)\n",
    "\n",
    "    X_train[cat_cols] = X_train[cat_cols].astype(\"category\")\n",
    "    X_valid[cat_cols] = X_valid[cat_cols].astype(\"category\")\n",
    "\n",
    "    if iter_cnt % 2 == 0:\n",
    "        model = lgb.LGBMClassifier(**params1)\n",
    "    else:\n",
    "        model = lgb.LGBMClassifier(**params2)\n",
    "\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=[(X_valid, y_valid)],\n",
    "        callbacks=[lgb.log_evaluation(100), lgb.early_stopping(100)],\n",
    "    )\n",
    "    fitted_models_lgb.append(model)\n",
    "\n",
    "    y_pred_valid = model.predict_proba(X_valid)[:, 1]\n",
    "    auc_score = roc_auc_score(y_valid, y_pred_valid)\n",
    "    cv_scores_lgb.append(auc_score)\n",
    "\n",
    "    iter_cnt += 1\n",
    "\n",
    "model = VotingModel(fitted_models_cat + fitted_models_lgb)\n",
    "\n",
    "print(f\"\\nCV AUC scores for CatBoost: {cv_scores_cat}\")\n",
    "print(f\"Maximum CV AUC score for Catboost: {max(cv_scores_cat)}\", end=\"\\n\\n\")\n",
    "\n",
    "\n",
    "print(f\"CV AUC scores for LGBM: {cv_scores_lgb}\")\n",
    "print(f\"Maximum CV AUC score for LGBM: {max(cv_scores_lgb)}\", end=\"\\n\\n\")\n",
    "\n",
    "del X, y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d4e4333",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T15:59:45.627847Z",
     "iopub.status.busy": "2024-05-23T15:59:45.627487Z",
     "iopub.status.idle": "2024-05-23T15:59:46.221473Z",
     "shell.execute_reply": "2024-05-23T15:59:46.220376Z"
    },
    "papermill": {
     "duration": 0.612231,
     "end_time": "2024-05-23T15:59:46.223579",
     "exception": false,
     "start_time": "2024-05-23T15:59:45.611348",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>case_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57543</th>\n",
       "      <td>0.010515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57549</th>\n",
       "      <td>0.040717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57551</th>\n",
       "      <td>0.006293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57552</th>\n",
       "      <td>0.035653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57569</th>\n",
       "      <td>0.119590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57630</th>\n",
       "      <td>0.011642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57631</th>\n",
       "      <td>0.022787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57632</th>\n",
       "      <td>0.013175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57633</th>\n",
       "      <td>0.025413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57634</th>\n",
       "      <td>0.016824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            score\n",
       "case_id          \n",
       "57543    0.010515\n",
       "57549    0.040717\n",
       "57551    0.006293\n",
       "57552    0.035653\n",
       "57569    0.119590\n",
       "57630    0.011642\n",
       "57631    0.022787\n",
       "57632    0.013175\n",
       "57633    0.025413\n",
       "57634    0.016824"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test: pd.DataFrame = df_test.drop(columns=[\"week_num\"]).set_index(\"case_id\")\n",
    "\n",
    "X_test[cat_cols] = X_test[cat_cols].astype(\"category\")\n",
    "\n",
    "y_pred: pd.Series = pd.Series(model.predict_proba(X_test)[:, 1], index=X_test.index)\n",
    "\n",
    "df_subm[\"score\"] = y_pred\n",
    "\n",
    "display(df_subm)\n",
    "\n",
    "df_subm.to_csv(\"submission.csv\")\n",
    "\n",
    "del X_test, y_pred\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30e6bc4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T15:59:46.253357Z",
     "iopub.status.busy": "2024-05-23T15:59:46.252963Z",
     "iopub.status.idle": "2024-05-23T15:59:47.554175Z",
     "shell.execute_reply": "2024-05-23T15:59:47.552899Z"
    },
    "papermill": {
     "duration": 1.318943,
     "end_time": "2024-05-23T15:59:47.556906",
     "exception": false,
     "start_time": "2024-05-23T15:59:46.237963",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!python /kaggle/usr/lib/script1/script1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a39518a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T15:59:47.588931Z",
     "iopub.status.busy": "2024-05-23T15:59:47.588540Z",
     "iopub.status.idle": "2024-05-23T15:59:48.231663Z",
     "shell.execute_reply": "2024-05-23T15:59:48.230219Z"
    },
    "papermill": {
     "duration": 0.661474,
     "end_time": "2024-05-23T15:59:48.233584",
     "exception": true,
     "start_time": "2024-05-23T15:59:47.572110",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/working/data.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df_train,y,df_test\u001b[38;5;241m=\u001b[39m\u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/kaggle/working/data.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/numpy_pickle.py:650\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    648\u001b[0m         obj \u001b[38;5;241m=\u001b[39m _unpickle(fobj)\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    651\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _read_fileobject(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m fobj:\n\u001b[1;32m    652\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    653\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[1;32m    654\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[1;32m    655\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/working/data.pkl'"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "df_train,y,df_test=joblib.load('/kaggle/working/data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63410d32",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fitted_models_lgb=[]\n",
    "model = lgb.LGBMClassifier()\n",
    "model.fit(df_train,y)\n",
    "fitted_models_lgb.append(model)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb68cd7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.base import RegressorMixin\n",
    "class VotingModel(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, estimators):\n",
    "        super().__init__()\n",
    "        self.estimators = estimators\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_preds = [estimator.predict(X) for estimator in self.estimators]\n",
    "        return np.mean(y_preds, axis=0)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        y_preds = [estimator.predict_proba(X) for estimator in self.estimators]\n",
    "        \n",
    "        return np.mean(y_preds, axis=0)\n",
    "\n",
    "model = VotingModel(fitted_models_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe01b2e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test = df_test.drop(columns=[\"WEEK_NUM\",'target'])\n",
    "df_test = df_test.set_index(\"case_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d63d3d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = pd.Series(model.predict_proba(df_test)[:,1], index=df_test.index)\n",
    "condition=y_pred<0.9855\n",
    "\n",
    "df_subm.loc[condition, 'score'] = (df_subm.loc[condition, 'score'] - 0.0735).clip(0)\n",
    "df_subm.to_csv(\"submission.csv\")\n",
    "df_subm\n",
    "!rm -rf data.pkl"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7921029,
     "sourceId": 50160,
     "sourceType": "competition"
    },
    {
     "sourceId": 178730808,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 668.277135,
   "end_time": "2024-05-23T15:59:49.471782",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-23T15:48:41.194647",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
